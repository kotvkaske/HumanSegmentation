{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1738d136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import models\n",
    "import torchvision.transforms as T\n",
    "from torchvision import models\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7743ac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import *\n",
    "from utils.dataset_custom import SemanticSegmentationDataset\n",
    "from utils.trainer_functions import train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539b4557",
   "metadata": {},
   "source": [
    "### Перед нами стоит задача предложить модель, сегментирующую человека на фотографии \n",
    "### Вход: фотография 320x240x3. Выход: маска человека 320x240. Метрика: Dice coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9b2be22",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda')\n",
    "image_size = (320,240)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f16426f",
   "metadata": {},
   "source": [
    "#### Помимо исходного датасеты были использованы также размеченные изображения из найденного репозитория^1. Затем датасет был увеличен за счет аугментации изображений (зеркальное отображение, добавление яркости и гауссовский шум, а также вращение - находятся в отдельном пайплайне). Далее все изображения приведены к одному размеру (Прим. для моделей на базе DeepLabv3, предобученных на ImageNet, используется также нормализация входных изображений)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df81e234",
   "metadata": {},
   "outputs": [],
   "source": [
    "trf = T.Compose([T.Resize(image_size),\n",
    "                 T.CenterCrop(image_size),\n",
    "                 T.ToTensor()])\n",
    "trf_rgb_imagenet = T.Compose([T.Resize(image_size),\n",
    "                              T.CenterCrop(image_size),\n",
    "                              T.ToTensor(),\n",
    "                              T.Normalize(mean = [0.485, 0.456, 0.406],\n",
    "                                          std = [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c119ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_paths('data/train/')+get_paths('data/augm_tr/')+get_paths('data/augm_extra_tr/')\n",
    "train_mask_data = get_paths('data/train_mask/')+get_paths('data/augm_tr_mask/')+get_paths('data/augm_extra_tr_mask/')\n",
    "val_data = get_paths('data/valid/')\n",
    "val_mask_data = get_paths('data/valid_mask/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41d4707f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - объектов 6124\n",
      "Val - объектов 145\n"
     ]
    }
   ],
   "source": [
    "print('Train - объектов',len(train_data))\n",
    "print('Val - объектов',len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76ee196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_custom = SemanticSegmentationDataset(train_data,trf,train_mask_data,trf)\n",
    "val_dataset_custom = SemanticSegmentationDataset(val_data,trf,val_mask_data,trf)\n",
    "train_dataloader_custom = DataLoader(train_dataset_custom,batch_size = 10,shuffle=True)\n",
    "val_dataloader_custom = DataLoader(val_dataset_custom,batch_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6626871",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_imagenet = SemanticSegmentationDataset(train_data,trf_rgb_imagenet,train_mask_data,trf)\n",
    "val_dataset_imagenet = SemanticSegmentationDataset(val_data,trf_rgb_imagenet,val_mask_data,trf)\n",
    "train_dataloader_imagenet = DataLoader(train_dataset_imagenet,batch_size = 10,shuffle=True)\n",
    "val_dataloader_imagenet = DataLoader(val_dataset_imagenet,batch_size = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216194ad",
   "metadata": {},
   "source": [
    "### Модель - для решения данной задачи я использовал SegNet (с нуля), кастомную версию UNET, а также предобученную модель DeeplabV3Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6924449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.custom_models import SegNet,DeepLabResnet\n",
    "from models.unet import UNET_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8ec45b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SegnetCustom = SegNet()\n",
    "#mymodel = torch.load('segnet_best.pth')\n",
    "optimizer_segnet = optim.Adam(params = SegnetCustom.parameters())\n",
    "#SegnetCustom = SegnetCustom.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df047901",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeeplabModel = DeepLabResnet()\n",
    "optimizer_deeplabmodel = optim.Adam(params = DeeplabModel.parameters())\n",
    "#DeeplabModel = DeeplabModel.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0fae247",
   "metadata": {},
   "outputs": [],
   "source": [
    "unetcustom = UNET_custom()\n",
    "optimizer_unet = optim.Adam(params = unetcustom.parameters())\n",
    "unetcustom = unetcustom.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9eb114",
   "metadata": {},
   "source": [
    "### Loss Functions - приведены стандартные лосс-функции для решения задачи сегментации. Для решения проблемы  несбалансированности классов используется Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cab82951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_loss(y_real, y_pred):\n",
    "    res = y_pred - y_pred*y_real + torch.log(1+torch.exp(-y_pred))  \n",
    "    return torch.mean(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5c18440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(y_pred, y_real, alpha=0.8, gamma=2):\n",
    "    y_pred = y_pred.view(-1)\n",
    "    y_real = y_real.view(-1)\n",
    "    BCE = bce_loss(y_pred, y_real)\n",
    "    BCE_EXP = torch.exp(-BCE)\n",
    "    loss = alpha * (1-BCE_EXP)**gamma * BCE\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7074af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(y_real, y_pred):      \n",
    "    y_pred = y_pred.view(-1)\n",
    "    y_real = y_real.view(-1)\n",
    "\n",
    "    intersection = (y_pred * y_real).sum()                            \n",
    "    dice = (2.*intersection)/(y_pred.sum() + y_real.sum())  \n",
    "\n",
    "    return 1 - dice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63079011",
   "metadata": {},
   "source": [
    "### Обучение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec3832d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train(SegnetCustom,optimizer_segnet,focal_loss,10,train_dataloader_custom,val_dataloader_custom,path_to_save= 'segnet_finalx2aug.pth')\n",
    "# train(DeeplabModel,optimizer_deeplabmodel,focal_loss,10,train_dataloader_imagenet,val_dataloader_imagenet,path_to_save= 'deeplabv3_finalx2aug.pth')\n",
    "# train(unetcustom,optimizer_unet,focal_loss,10,train_dataloader_custom,val_dataloader_custom,path_to_save= 'unet_finalx2aug.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1370a24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SegnetCustom = torch.load('segnet_finalx2aug.pth')\n",
    "DeeplabModel = torch.load('deeplabv3_finalx2aug.pth')\n",
    "UnetCustom = torch.load('unet_finalx2aug.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972d8fab",
   "metadata": {},
   "source": [
    "### Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc97cc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.metrics import *\n",
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56c07c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_segnet = list(predict(SegnetCustom,val_dataloader_custom))\n",
    "res_deeplab = list(predict(DeeplabModel,val_dataloader_imagenet))\n",
    "res_unet = list(predict(UnetCustom,val_dataloader_custom))\n",
    "val_true = [np.array(i[1]) for i in val_dataset_custom]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726fd73e",
   "metadata": {},
   "source": [
    "#### Dice Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32636bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8962681481750732"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dice(res_unet,val_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4a789f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9533790059716286"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dice(res_deeplab,val_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f65269c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.923738732065178"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dice(res_segnet,val_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45e3b85",
   "metadata": {},
   "source": [
    "#### IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80ba4d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8495632615582697"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_model(DeeplabModel, iou_pytorch, val_dataloader_imagenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c7149ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.741560363563998"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_model(SegnetCustom, iou_pytorch, val_dataloader_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ac482ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.664548869790702"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_model(UnetCustom, iou_pytorch, val_dataloader_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9398c8",
   "metadata": {},
   "source": [
    "### Таким образом, модель на базе DeepLabv3Resnet50 обладает наилучшей сегментирующей способностью, поэтому используем ее для решения задачи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dedb6a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_paths('data/test/')\n",
    "test_dataset_imagenet = SemanticSegmentationDataset(test,transform_x = trf_rgb_imagenet,type_of_data='test')\n",
    "test_dataloader = DataLoader(test_dataset_imagenet,batch_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce859c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_submission = list(predict(DeeplabModel,test_dataloader,type_of_data='test'))\n",
    "test_submission_v2 = [i.astype(np.uint8) for i in test_submission]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b993c5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "109bfe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "rle_mask = [encode_rle(test_submission[i]) for i in range(len(test_submission))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "510afa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_names = get_paths('data/test/',str_format=False)\n",
    "test_index = [int(i.name.split('.')[0]) for i in test_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88d27931",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=rle_mask,index=test_index).reset_index().rename(columns={'index':'id',0:'rle_mask'}).to_csv('final_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b64349e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>rle_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1460</td>\n",
       "      <td>66 56 306 56 546 56 786 56 1025 57 1263 61 150...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1461</td>\n",
       "      <td>5660 3 5890 31 6125 42 6360 52 6597 58 6834 62...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1462</td>\n",
       "      <td>71 121 311 121 551 121 791 121 1031 121 1270 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1463</td>\n",
       "      <td>8234 4 8251 6 8469 12 8488 14 8555 5 8706 39 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1464</td>\n",
       "      <td>2748 2 2979 19 3215 31 3452 38 3689 45 3927 49...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1555</td>\n",
       "      <td>4195 14 4431 23 4667 43 4902 51 5139 56 5376 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1556</td>\n",
       "      <td>122 119 362 119 602 119 842 119 1082 119 1321 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1557</td>\n",
       "      <td>1 159 241 159 481 159 721 159 961 159 1201 160...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1558</td>\n",
       "      <td>7327 30 7562 39 7797 48 8034 54 8271 60 8508 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1559</td>\n",
       "      <td>26697 5 26928 21 27155 41 27388 52 27627 56 27...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                           rle_mask\n",
       "0   1460  66 56 306 56 546 56 786 56 1025 57 1263 61 150...\n",
       "1   1461  5660 3 5890 31 6125 42 6360 52 6597 58 6834 62...\n",
       "2   1462  71 121 311 121 551 121 791 121 1031 121 1270 1...\n",
       "3   1463  8234 4 8251 6 8469 12 8488 14 8555 5 8706 39 8...\n",
       "4   1464  2748 2 2979 19 3215 31 3452 38 3689 45 3927 49...\n",
       "..   ...                                                ...\n",
       "95  1555  4195 14 4431 23 4667 43 4902 51 5139 56 5376 6...\n",
       "96  1556  122 119 362 119 602 119 842 119 1082 119 1321 ...\n",
       "97  1557  1 159 241 159 481 159 721 159 961 159 1201 160...\n",
       "98  1558  7327 30 7562 39 7797 48 8034 54 8271 60 8508 6...\n",
       "99  1559  26697 5 26928 21 27155 41 27388 52 27627 56 27...\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('final_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51e7143b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_names = get_paths('data/test/')\n",
    "ch = get_html(test_names, test_submission_v2, path_to_save=\"final_submit/pics\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
